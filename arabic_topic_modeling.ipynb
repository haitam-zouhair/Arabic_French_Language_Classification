{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "arabic_topic_modeling.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/haitam-zouhair/workspace_henceforth/blob/master/arabic_topic_modeling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9jPq_zHjSVqc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "bfea4c6f-d05a-407d-dd04-e65ac13eca0f"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B7BYIdWFLsKB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "01390173-c810-4761-c806-672833c3e6e6"
      },
      "source": [
        "%%shell\n",
        "cp -r 'drive/My Drive/Projects/Zouhair/' ."
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cp: cannot stat 'drive/My Drive/Projects/Zouhair/': No such file or directory\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "CalledProcessError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-9a0e36172d30>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'shell'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"cp -r 'drive/My Drive/Projects/Zouhair/' .\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2115\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2116\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2117\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2118\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_shell_cell_magic\u001b[0;34m(args, cmd)\u001b[0m\n\u001b[1;32m    110\u001b[0m   \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_run_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclear_streamed_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mparsed_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mignore_errors\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m     \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_returncode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36mcheck_returncode\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m       raise subprocess.CalledProcessError(\n\u001b[0;32m--> 138\u001b[0;31m           returncode=self.returncode, cmd=self.args, output=self.output)\n\u001b[0m\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_repr_pretty_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcycle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=unused-argument\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mCalledProcessError\u001b[0m: Command 'cp -r 'drive/My Drive/Projects/Zouhair/' .' returned non-zero exit status 1."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KKUsX-XCP6hl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.chdir(\"Zouhair\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0f5lilJaAIh4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install pyLDAvis"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wTagrZm30cBP",
        "colab_type": "text"
      },
      "source": [
        "# Dataset\n",
        "\n",
        "I used [SANAD](https://www.sciencedirect.com/science/article/pii/S2352340919304305) as dataset. Dataset can be downloaded [here](https://data.mendeley.com/datasets/57zpx667y9/2), you can also use directly the dataset from Google Drive, I put them [here](https://drive.google.com/file/d/1hjDR0C5vHPckZisqYby13r2m1i5mtH3p/view?usp=sharing)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5PLiFxQHQA0W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%shell\n",
        "unzip arabic.zip\n",
        "unzip Akhbarona.zip -d Akhbarona\n",
        "unzip Arabiya.zip -d Arabiya\n",
        "unzip Khaleej.zip -d Khaleej\n",
        "unzip SANAD_SUBSET.zip -d SANAD_SUBSET\n",
        "rm *.zip "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kndoy2OJTHxb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "\n",
        "DATANAME = \"Arabiya\"\n",
        "\n",
        "data_frame = pd.DataFrame(columns=[\"text\", \"label\"])\n",
        "\n",
        "for cat in os.listdir(DATANAME):\n",
        "    print(\"reading files from category {}...\".format(cat))\n",
        "    dir_path = os.path.join(DATANAME, cat)\n",
        "\n",
        "    for file_name in os.listdir(dir_path):\n",
        "        path = os.path.join(dir_path, file_name)\n",
        "        \n",
        "        with open(path, \"r\") as f:\n",
        "            text = f.read()\n",
        "\n",
        "        data_frame = data_frame.append({\"text\": text, \"label\": cat},\n",
        "                                       ignore_index=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P7cKmkwSl8kf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(len(data_frame))\n",
        "data_frame.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K20yWVYlmElG",
        "colab_type": "text"
      },
      "source": [
        "# I/-Arabic Text Processing\n",
        "\n",
        "Similar to English text, arabic text should be preporcessed before performing any NLP task on it. The pre-processing steps are roughly the same:\n",
        "\n",
        "* **Tokenization:**\n",
        "* **Remove Stopwords:** Arabic stop words can be found [here](https://github.com/mohataher/arabic-stop-words/blob/master/list.txt).\n",
        "* **Lemmatization**:  I am not sure how to lemmatize arabic text, I think in arabic steeming and lemmatization are the same text. I have found this [paper](https://www.aclweb.org/anthology/L18-1181.pdf)  but we need to check this later to be sure (**TO DO**).\n",
        "* **Stemming**: We use `nltk.SRIStemmer`\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5rKWflJOuJCT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%shell\n",
        "#Download arabic stop words\n",
        "wget https://raw.githubusercontent.com/mohataher/arabic-stop-words/master/list.txt\n",
        "mv list.txt arabic_stop_words.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V00D7Ac8vsP2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from string import punctuation\n",
        "from nltk import ISRIStemmer\n",
        "\n",
        "\n",
        "punctuation += '،؛؟”0123456789“'\n",
        "stop_words = open(\"arabic_stop_words.txt\").read().splitlines()\n",
        "stemmer = ISRIStemmer()\n",
        "\n",
        "def preprocess(text):\n",
        "    # tokenize\n",
        "    text = ''.join(c for c in text if c not in punctuation)\n",
        "    tokens = text.split()  \n",
        "\n",
        "    # remove stop words   \n",
        "    tokens = [w for w in tokens if w not in stop_words]\n",
        "\n",
        "    # stem\n",
        "    stems = [stemmer.stem(w) for w in tokens]\n",
        "\n",
        "    return stems\n",
        "\n",
        "\n",
        "processed_docs = data_frame['text'].map(preprocess)\n",
        "data_frame['processed_text'] = processed_docs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AjhVn8FpzpB3",
        "colab_type": "text"
      },
      "source": [
        "# II/- Topic modeling using LDA\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F0-VML4r5X1G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from gensim import corpora, models\n",
        "\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MuMV2XuU2z1A",
        "colab_type": "text"
      },
      "source": [
        "## 1. Bag of Words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0eiPstq1zv9l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# build bag of words dictionnary\n",
        "dictionary = corpora.Dictionary(data_frame['processed_text'])\n",
        "\n",
        "# filter iinfrequent words\n",
        "dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)\n",
        "bow_corpus = [dictionary.doc2bow(doc) for doc in data_frame['processed_text']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bp4K3rlX44nv",
        "colab_type": "text"
      },
      "source": [
        "View Bag of Word result"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4V0ANEKA4d_v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(len(bow_corpus[0])):\n",
        "    print(\"Word {} (\\\"{}\\\") appears {} time.\".format(bow_corpus[0][i][0], \n",
        "                                               dictionary[bow_corpus[0][i][0]], \n",
        "                                               bow_corpus[0][i][1]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CoKcfFIa5pO-",
        "colab_type": "text"
      },
      "source": [
        "## 2. Running LDA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SDN9hsaA54P1",
        "colab_type": "text"
      },
      "source": [
        "Our dataset has $6$ topics, we will try to build LDA model with $6$ topics."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fHODfbEm5q3R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "NUM_TOPICS = 6\n",
        "\n",
        "lda_model = models.LdaMulticore(bow_corpus,\n",
        "                                num_topics=NUM_TOPICS,\n",
        "                                id2word=dictionary,\n",
        "                                passes=2,\n",
        "                                workers=4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eIAIZU5r65W_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for idx, topic in lda_model.print_topics(-1):\n",
        "    print('Topic: {} | Word: {}'.format(idx, topic))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kxNPr2cp8UCO",
        "colab_type": "text"
      },
      "source": [
        "Seems that we can recognize some topics from those results\n",
        "\n",
        "* Topic 0 corresponds to **Finance**\n",
        "* Topic 1 corresponds to **Sports**\n",
        "* Topic 2 corresponds to **Culture**\n",
        "* Topic 3 corresponds to **Tech**   \n",
        "* Topic 4 correnponds to **Finance**\n",
        "* Topic 5 corresponds to **Politics**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gITBM-2rN3Xg",
        "colab_type": "text"
      },
      "source": [
        "# III/- Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zM71mPj5_Iz8",
        "colab_type": "text"
      },
      "source": [
        "## 1) Vizualisation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "paXpgT_D-XFn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pyLDAvis.gensim\n",
        "import pickle \n",
        "import pyLDAvis\n",
        "\n",
        "pyLDAvis.enable_notebook()\n",
        "LDAvis_prepared = pyLDAvis.gensim.prepare(lda_model, bow_corpus, dictionary)\n",
        "LDAvis_prepared"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NIStC1V6AUbs",
        "colab_type": "text"
      },
      "source": [
        "## 2. Perplexity and Coherence Score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bw037WvvAY1Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "coherence_model_lda = models.CoherenceModel(model=lda_model,\n",
        "                                     texts=data_frame[\"processed_text\"],\n",
        "                                     dictionary=dictionary,\n",
        "                                     coherence='c_v')\n",
        "\n",
        "print('Coherence Score: {:.2f}'.format(coherence_model_lda.get_coherence()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j4b6WElbBllO",
        "colab_type": "text"
      },
      "source": [
        "# IV/- Content classification using topic modeling\n",
        "\n",
        "In this section, we use the results of topic modeling in order to build a content classifier. Each document will be represented by a `NUM_TOPICS`-vector capturing the distribution of the learnt topics. This is similar to the approach used in [Phan et al. (2008)](http://gibbslda.sourceforge.net/fp224-phan.pdf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T6dzSZvvG8ZF",
        "colab_type": "text"
      },
      "source": [
        "## 1. Build dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QdCr4vhIEw9X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "N = len(bow_corpus)\n",
        "\n",
        "\n",
        "X = np.zeros((N, NUM_TOPICS))\n",
        "y = np.zeros(N, dtype=np.uint8)\n",
        "\n",
        "\n",
        "label2id = {label: idx for (idx, label)\n",
        " in enumerate(list(set(data_frame[\"label\"])))}\n",
        "\n",
        "for i in range(N):\n",
        "    topics_proba = lda_model.get_document_topics(bow_corpus[i],\n",
        "                                              minimum_probability=0.0)\n",
        "    \n",
        "    topic_probas = [v for _, v in  topics_proba] \n",
        "    X[i,:] = np.array(topic_probas)\n",
        "    y[i] = label2id[data_frame[\"label\"][i]]\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CthILMKlG-GF",
        "colab_type": "text"
      },
      "source": [
        "## 2. Train classifier\n",
        "\n",
        "We train some simple classifiers using the extracted feature vectors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CHdFUIi6HAwi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "68ed266b-5189-4b46-c370-090b4fcba6f8"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "# Train/Test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)\n",
        "\n",
        "# SVM\n",
        "svm_clf = SVC()\n",
        "svm_clf.fit(X_train, y_train)\n",
        "\n",
        "train_acc = svm_clf.score(X_train, y_train)\n",
        "test_acc = svm_clf.score(X_test, y_test)\n",
        "print(\"SVM: Train accuracy {0:.2f} | Test acuracy {1:.2f}\".format(train_acc,\n",
        "                                                                  test_acc))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SVM: Train accuracy 0.87 | Test acuracy 0.87\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4JEFNFqMQaXR",
        "colab_type": "text"
      },
      "source": [
        "# V/- Model Deployment\n",
        "\n",
        "In this section we save the model, and we load it into `lda_model_uploaded` and test it on new text.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DEBE1OgZQeBy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "\n",
        "model_path = \"model\"\n",
        "\n",
        "lda_model.save(model_path)\n",
        "lda_model_loaded = lda_model.load(model_path)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PuPMmmjwOFTP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle\n",
        "\n",
        "with open(\"arab_class.pkl\", \"wb\") as f:\n",
        "    pickle.dump(svm_clf, f, pickle.HIGHEST_PROTOCOL)"
      ],
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A_C3H7Yn7SSU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "clf = pickle.load(open(\"arab_class.pkl\",\"rb\"))"
      ],
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JOwVBP34YJPe",
        "colab_type": "text"
      },
      "source": [
        "Consider some text, for example"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zqK7NVdEXsf5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "f73a8319-724d-4716-9860-35759d03ec5e"
      },
      "source": [
        "#get a random text\n",
        "text = data_frame.sample()[\"text\"].values[0]\n",
        "\n",
        "print(text)\n",
        "\n",
        "# preprocess text\n",
        "processed_text = preprocess(text)\n",
        "\n",
        "# get Bag of Words\n",
        "bow_doc = lda_model_loaded.id2word.doc2bow(processed_text)\n",
        "\n",
        "# get topics \n",
        "topics = lda_model_loaded.get_document_topics(bow_doc,\n",
        "                                              minimum_probability=0.0)\n",
        "topic = [v for _, v in  topics] \n",
        "query= np.array(topic)\n",
        "\n",
        "result=clf.predict([query])\n",
        "A=['Culture', 'Tech', 'Politics', 'Finance', 'Medical', 'Sports']\n",
        "print(A[result[0]])\n",
        "print(\"*\"*30)\n",
        "\n",
        "print([topics])"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "أعلن في بغداد عن أسماء الأديبات الفائزات بجائزة \"نازك الملائكة للإبداع الأدبي\" النسوي في دورتها السابعة (دورة 2014)، وتميّزت دورة هذا العام عن الدورات السابقة بسعة المشاركة وتنوّع البلدان؛ حيث تلقت اللجنة التحكيمية 156 مشاركة من داخل وخارج العراق؛ وتناولت المشاركات مواضيع عدة وكلّ ضمن الحقل المختصة فيه. والملاحظة التي يمكن تأشيرها في هذه الدورة هو فوز الأديبات العربيات في مجالي الشعر والقصة؛ تاركات للأديبات العراقيات جوائز النقد. فجائزة الشعر الأولى ذهبت للشاعرة اللبنانية نور حيدر، والثانية للشاعرة السورية ليندا سلمان ابراهيم؛ والجائزة الثالثة للشاعرة المغربية صباح الدبى. أما في مجال القصة القصيرة فكانت الجائزة الأولى للقاصة العراقية إيمان راضي عبد الحسين، والفائزة بالجائزة الثانية القاصة زينة بو رويسة من الجزائر، في الوقت الذي كان المركز الثالث من نصيب القاصة المصرية غادة محمد العبسي. وجاءت جوائز النقد العراقية كالآتي (ماجدة هاتو بالمركز الأول) و(نادية هناوي سعدون بالمركز الثاني) بينما كان المركز الثالث من نصيب (فاطمة بدر حسين). ويذكر أن جائزة نازك الملائكة للإبداع النسوي أسستها وزارة الثقافة عام 2008 لتعنى بالأدب النسوي، وكانت الدورات الثلاث الأولى مقتصرة على الشعر وللأقلام العراقية فقط، إلا أنها منذ الدورة الرابعة انفتحت عربيا؛ لتصبح ظاهرة أدبية تليق باسم الشاعرة الرائدة الملائكة وبكشوفاتها الشعرية والنقدية. وسيتم توزيع الجوائز بمشاركة الأديبات الفائزات وحضور جمهرة من الأدباء والإعلاميين يوم السبت الموافق 18 أكتوبر 2014 وعلى قاعة قرطبة في فندق المنصور ميليا.\n",
            "Culture\n",
            "******************************\n",
            "[[(0, 0.0010398097), (1, 0.0010411317), (2, 0.565715), (3, 0.1834822), (4, 0.24768244), (5, 0.0010394776)]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xDw6VYSwanN7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for idx, topic in lda_model.print_topics(-1):\n",
        "    print('Topic: {} | Word: {}'.format(idx, topic))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dBl7FiKEFvpR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "ccef067c-ba21-4cca-e134-3a6240c4903b"
      },
      "source": [
        "data_frame[\"label\"]\n"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0        Culture\n",
              "1        Culture\n",
              "2        Culture\n",
              "3        Culture\n",
              "4        Culture\n",
              "          ...   \n",
              "71241     Sports\n",
              "71242     Sports\n",
              "71243     Sports\n",
              "71244     Sports\n",
              "71245     Sports\n",
              "Name: label, Length: 71246, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1RM7qBl8FyvY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "bacd0175-50f3-4a7f-dbe6-e260dbac2317"
      },
      "source": [
        "result[0]"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hTvUZyodGVK1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "ad1e3535-3105-4393-aad9-0fcb8fd21111"
      },
      "source": [
        "A"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0         Culture\n",
              "5619         Tech\n",
              "10029    Politics\n",
              "14397     Finance\n",
              "44473     Medical\n",
              "48188      Sports\n",
              "Name: label, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GFlD_HdUKsVy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "28fa8583-0e9a-4a04-a246-27a100012fcc"
      },
      "source": [
        "A[0]"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Culture'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j4HYN_bcLI3y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! cp \"/content/Zouhair/arab_class.pkl\" \"/content/drive/My Drive/Projects/Zouhair/Models/\""
      ],
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8bunW7E7mZkZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! cp \"/content/Zouhair/model\" \"/content/drive/My Drive/Projects/Zouhair/Models/\""
      ],
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AyhjKPyEmp7y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! cp \"/content/Zouhair/model.expElogbeta.npy\" \"/content/drive/My Drive/Projects/Zouhair/Models/\""
      ],
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zn-h5YW_mzNh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! cp \"/content/Zouhair/model.id2word\" \"/content/drive/My Drive/Projects/Zouhair/Models/\""
      ],
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MlyKyZYRm9f5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! cp \"/content/Zouhair/model.state\" \"/content/drive/My Drive/Projects/Zouhair/Models/\""
      ],
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sHmjhRKgnFyE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lda_model_loaded = lda_model.load(\"/content/drive/My Drive/Projects/Zouhair/Models/model\")\n",
        "clf = pickle.load(open(\"/content/drive/My Drive/Projects/Zouhair/Models/arab_class.pkl\",\"rb\"))\n",
        "\n",
        "#get a random text\n",
        "text = \" هو فريق كرة قدم محترف إسباني أُسس عام 1902، مقره العاصمة الإسبانية مدريد. يلعب الفريق في الدوري الإسباني واختير كأفضل فريق كرة قدم في القرن العشرين، وقد فاز بالدوري الإسباني 33 مرة (رقم قياسي)، وتسعة عشر مرة بكأس ملك إسبانيا وأحرز رقماً قياسياً بحيازته 13 بطولة في دوري أبطال أوروبا \"\n",
        "\n",
        "print(text)\n",
        "\n",
        "# preprocess text\n",
        "processed_text = preprocess(text)\n",
        "\n",
        "# get Bag of Words\n",
        "bow_doc = lda_model_loaded.id2word.doc2bow(processed_text)\n",
        "\n",
        "# get topics \n",
        "topics = lda_model_loaded.get_document_topics(bow_doc,\n",
        "                                              minimum_probability=0.0)\n",
        "topic = [v for _, v in  topics] \n",
        "query= np.array(topic)\n",
        "\n",
        "result=clf.predict([query])\n",
        "A=['Culture', 'Tech', 'Politics', 'Finance', 'Medical', 'Sports']\n",
        "print(A[result[0]])\n",
        "print(\"*\"*30)\n",
        "\n",
        "print([topics])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pZLMTeuRodZA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}